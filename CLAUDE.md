# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Aegis is an event-driven, cloud-native AIOps framework for Kubernetes that provides automated operations, intelligent diagnostics, and health checks for AI-HPC clusters. The system converts alerts into Kubernetes custom resources and triggers automated remediation workflows through Argo Workflows.

## Architecture

### Core Components

**Controller (cmd/aegis/main.go)**
- Main control loop implementing the Kubernetes operator pattern
- Manages multiple controllers for different CRD types (Alert, Rule, Template, Diagnosis, HealthCheck)
- Runs HTTP API server for webhook-based alert ingestion
- Integrates with AI providers for alert parsing and diagnosis

**CLI Tool (cmd/aegis-cli/main.go)**
- Command-line interface for managing Aegis resources
- Subcommands: `auth` (authentication), `rule` (rule management)
- Uses kubeconfig for cluster authentication

**Selfhealing Component (cmd/aegis-selfhealing/main.go)**
- Standalone service for self-healing operations
- Works alongside the main controller

### Custom Resource Definitions (CRDs)

All CRDs are defined in `pkg/apis/*/v1alpha1/` and follow the Kubernetes API conventions:

- **AegisAlert** (`pkg/apis/alert/v1alpha1/`) - Represents alerts from various sources (Alertmanager, custom webhooks, AI-parsed)
- **AegisAlertOpsRule** (`pkg/apis/rule/v1alpha1/`) - Defines matching rules for alerts and references to operation templates
- **AegisOpsTemplate** (`pkg/apis/template/v1alpha1/`) - Contains Argo Workflow templates for automated remediation
- **AegisDiagnosis** (`pkg/apis/diagnosis/v1alpha1/`) - Manages LLM-based diagnostics for Nodes, Pods, and PytorchJobs
- **AegisNodeHealthCheck** (`pkg/apis/nodecheck/v1alpha1/`) - Periodic health checks for nodes
- **AegisClusterHealthCheck** (`pkg/apis/clustercheck/v1alpha1/`) - Cluster-wide health checks

### Key Packages

**internal/controller/** - Controller implementations
- Alert lifecycle management and status tracking
- Rule matching engine for alert-to-workflow mapping
- Diagnosis orchestration (spawns collector pods, aggregates data, calls LLM)
- Health check scheduling and execution
- `nodepoller/` - Active polling of node metrics from Prometheus

**internal/device_aware/** - Device-aware scheduling and condition management
- Priority-based condition configuration
- GPU/device-specific health tracking

**pkg/analyzer/** - Analysis engines for different resource types
- Node, Pod, PytorchJob analyzers
- Data collection and formatting for LLM diagnosis
- `common/` contains shared analysis utilities

**pkg/ai/** - AI provider integrations
- OpenAI, Vertex AI, AWS Bedrock, Ollama, and other LLM providers
- Alert parsing and diagnostic explanation generation
- Factory pattern for provider instantiation

**api/** - HTTP API handlers
- `/ai/alert` - AI-powered alert parsing endpoint
- `/alertmanager/alert` - Alertmanager webhook receiver
- `/alert` - Custom JSON alert format
- Alert deduplication and conversion to CRDs

**pkg/generated/** - Auto-generated Kubernetes client code (DO NOT EDIT MANUALLY)
- Generated by `hack/update-codegen.sh`
- Clientsets, informers, and listers for all CRDs

## Common Development Commands

### Building

Build controller image:
```bash
docker build -t aegis:test -f Dockerfile .
```

Build selfhealing image:
```bash
docker build -t aegis-selfhealing:test -f Dockerfile.selfhealing .
```

The Dockerfile builds both `aegis` (controller) and `aegiscli` binaries in a multi-stage build.

### Code Generation

After modifying CRD types in `pkg/apis/*/v1alpha1/types.go`:
```bash
./hack/update-codegen.sh
```

This generates clientsets, informers, listers, and deepcopy methods.

Update CRD manifests:
```bash
./hack/verify-crdgen.sh
```

Update OpenAPI spec:
```bash
./hack/update-generated-openapi.sh
```

### Testing

Run unit tests:
```bash
go test -mod=vendor ./cmd/... ./pkg/... ./internal/...
```

The provided `hack/unit-test.sh` and `hack/integration-test.sh` are templates from kubernetes/scheduler-plugins and need to be updated for Aegis-specific tests.

### Deployment

Install CRDs:
```bash
kubectl apply -f manifests/install/
```

Deploy controller via raw manifests:
```bash
kubectl apply -f deploy/ -n monitoring
```

Deploy via Helm:
```bash
helm install aegis ./deploy/aegis-helm -n monitoring
```

Helm chart location: `deploy/aegis-helm/`
- Current chart version: 3.1.0
- Includes controller deployment, RBAC, CRDs, and optionally selfhealing component

### Vendoring

Update vendored dependencies:
```bash
./hack/update-vendor.sh
```

Or manually:
```bash
go mod tidy
go mod vendor
```

## Code Patterns and Conventions

### Controller Pattern

All controllers follow the Kubernetes controller pattern:
1. Watch CRD resources via informers
2. Enqueue changed objects to workqueue
3. Process queue items in worker goroutines
4. Reconcile desired vs actual state
5. Update resource status

Example in `internal/controller/aegis.go`:
```go
controller.AddEventHandler(cache.ResourceEventHandlerFuncs{
    AddFunc: ctrl.enqueue,
    UpdateFunc: func(old, new interface{}) { ctrl.enqueue(new) },
    DeleteFunc: ctrl.enqueue,
})
```

### Template Rendering

Aegis uses Go templates (`text/template`) to render Argo Workflow specs. Alert details are injected as template variables:
```yaml
spec:
  manifest: |
    # Argo Workflow spec with {{.node}}, {{.pod}}, etc.
```

See `pkg/controller/rule_engine_util.go` for rendering logic.

### Status Updates

CRD status fields track lifecycle:
- AegisAlert: `Firing`/`Resolved`, ops status `Pending`/`Running`/`Succeeded`/`Failed`
- AegisDiagnosis: `Pending`/`Diagnosing`/`Completed`/`Failed`

Always update status in a separate goroutine with retry logic to avoid blocking the reconciliation loop.

### AI Integration

When adding new AI providers:
1. Implement the `ai.Provider` interface in `pkg/ai/`
2. Register in the factory (`pkg/ai/factory.go`)
3. Configure via `--ai.provider` flag

### Diagnosis Flow

1. User creates AegisDiagnosis CR specifying object (Node/Pod/PytorchJob)
2. Controller spawns a collector pod (if needed for Node diagnosis)
3. Analyzer gathers logs, events, metrics, and Prometheus data
4. Data formatted into prompt and sent to LLM
5. Response written to AegisDiagnosis status

Custom prompts can be configured via ConfigMap (see docs/diagnosis-custom-prompt-guide.md).

### Alert Deduplication

Alerts are deduplicated by fingerprint. The system maintains a cache of recent alerts to avoid creating duplicate AegisAlert resources. See `api/apis/alert.go`.

## Important Files

- `pkg/apis/*/v1alpha1/types.go` - CRD type definitions
- `internal/controller/aegis.go` - Main controller orchestration
- `internal/controller/lifecycle.go` - Alert lifecycle management
- `internal/controller/status.go` - Status update logic
- `api/apis/alert.go` - Alert webhook handlers and parsing
- `pkg/ai/factory.go` - AI provider factory
- `deploy/aegis-helm/values.yaml` - Helm chart configuration
- `manifests/install/*.yaml` - CRD definitions

## Configuration

Controller is configured via CLI flags (see `cmd/aegis/main.go` parse() function):

Key flags:
- `--kubeconfig` - Path to kubeconfig
- `--http-port` - API server port (default: 80)
- `--alert.publish-namespace` - Namespace for AegisAlert resources
- `--diagnosis.enable` - Enable diagnosis controller
- `--diagnosis.explain` - Enable LLM-based explanation
- `--diagnosis.collector-image` - Collector pod image for node diagnosis
- `--ai.provider` - AI backend (openai, vertex, ollama, etc.)
- `--enable-leader-election` - Enable HA mode
- `--prometheus.endpoint` - Prometheus endpoint for metrics

Configuration can also be loaded from a YAML file via `--config` flag. System-wide parameters can be set via `alert.system-parameters` map in the config file.

## Helm Chart Structure

`deploy/aegis-helm/` contains consolidated chart for both controller and selfhealing:
- `templates/controller-*.yaml` - Controller deployment, service, RBAC
- `templates/selfhealing-*.yaml` - Selfhealing DaemonSet (optional)
- `crds/` - CRD definitions (auto-installed)
- `values.yaml` - Configurable parameters

The chart was recently consolidated (v3.1.0) to include selfhealing functionality previously in a separate chart.

## Diagnostic Capabilities

Aegis supports three object types for diagnosis:
- **Node**: Spawns collector pod to gather node-level metrics, dmesg, journal logs
- **Pod**: Analyzes pod events, logs from all containers, and related resources
- **PytorchJob**: Specialized analysis for distributed training jobs

See `pkg/analyzer/` for implementation details. Each analyzer implements the `Analyzer` interface.

## Health Checks

- **AegisNodeHealthCheck**: Runs periodic checks on nodes from pod context (useful for simulating production workloads in AI/HPC)
- **AegisClusterHealthCheck**: Cluster-wide checks

Unlike node-problem-detector, Aegis supports checks from within pods, which is valuable for GPU/accelerator validation.

## Node Poller (Experimental)

The node poller (`internal/controller/nodepoller/`) actively queries Prometheus for node metrics and generates synthetic alerts when thresholds are exceeded. Useful for proactive problem detection before Alertmanager rules fire.

Enable with `--node-poller.enable=true` and configure Prometheus endpoint.

## Device-Aware Features (Experimental)

Device-aware mode (`--device-aware.enable=true`) adds priority-based condition management for GPU/accelerator nodes. Configuration is data-driven via ConfigMap, supporting per-condition flags like `AffectsLoad` and `DeviceIDMode`.
